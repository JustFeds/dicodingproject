# -*- coding: utf-8 -*-
"""dapd_fh.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/JustFeds/dicodingproject/blob/main/dapd_fh.ipynb

---



---


# Data Analytics Project: eCommerce_Public
- Nama                  : Fitroh Harris
- Email                 : fitrohharris@gmail.com
- Id Dicoding           : justfeds

---



---

---


## Business Questions/ Problem Statements


---

- Business performance;
- Revenue growth;
- Customer distribution;
- Product performance.

---


## Libraries


---

**Please make sure you run all this library before run another scripts**
"""

import  numpy              as np
import  pandas             as pd
import  matplotlib         as mpl
import  matplotlib.pyplot  as plt
import  seaborn            as sns

"""---


## Data Wrangling


---

### Gathering Data

This environment was made on cloud computing, means everyone could see any update or run this scripting live on [link text](https://colab.research.google.com/drive/1-VEGZByAsesVGywpILU9UfHaSccdMdtS?usp=sharing)

Before you re-run this scripts please make sure you already copy dataset on [link text](https://drive.google.com/drive/folders/1y6zCVAyNe348cy8n6IzYLcFnDq35Dtcq?usp=sharing) to your google drive and save on folder with this format **Datasets/eCommerce_Public/customers_dataset.csv**

This is how to import data from google drive
"""

from google.colab import drive

"""This is how to mounting your google drive into this enviroment"""

drive.mount('/content/drive')

"""This is how to pathcing you data"""

path_customers = '/content/drive/My Drive/Datasets/eCommerce_Public/customers_dataset.csv'

"""This is how pandas could path your data, on this step we make pandas read all dataset"""

customers_df = pd.read_csv(path_customers)
customers_df.head()

path_geo = '/content/drive/My Drive/Datasets/eCommerce_Public/geolocation_dataset.csv'

geo_df = pd.read_csv(path_geo)
geo_df.head()

path_OrderItems = '/content/drive/My Drive/Datasets/eCommerce_Public/order_items_dataset.csv'

OrderItems_df = pd.read_csv(path_OrderItems)
OrderItems_df.head()

path_OrderPayments = '/content/drive/My Drive/Datasets/eCommerce_Public/order_payments_dataset.csv'

OrderPayments_df = pd.read_csv(path_OrderPayments)
OrderPayments_df.head()

path_OrderReviews = '/content/drive/My Drive/Datasets/eCommerce_Public/order_reviews_dataset.csv'

OrderReviews_df = pd.read_csv(path_OrderReviews)
OrderReviews_df.head()

path_Orders = '/content/drive/My Drive/Datasets/eCommerce_Public/orders_dataset.csv'

Orders_df = pd.read_csv(path_Orders)
Orders_df.head()

path_ProductCat = '/content/drive/My Drive/Datasets/eCommerce_Public/product_category_name_translation.csv'

ProductCat_df = pd.read_csv(path_ProductCat)
ProductCat_df.head()

path_Products = '/content/drive/My Drive/Datasets/eCommerce_Public/products_dataset.csv'

Products_df = pd.read_csv(path_Products)
Products_df.head()

path_Sellers = '/content/drive/My Drive/Datasets/eCommerce_Public/sellers_dataset.csv'

Sellers_df = pd.read_csv(path_Sellers)
Sellers_df.head()

"""### Data Assesment

On this step we assessing data:

with [  **.info()** ] so we could see data type;

with [  **.nunique()**  ] so we could distinct values in a column;

with [  **.isna().sum()** ] so we could give a Series where the index is the column names, and the values are the counts of missing values in each column;

With [  **.duplicated().sum()** ] so we could give the count of rows that have duplicates in the DataFrame;

With [  **.describe()** ] so we could quickly understanding the distribution of data and identifying potential issues, such as outliers or missing values.
"""

customers_df.info()

customers_df.nunique()

print("Total Duplication: ",customers_df.duplicated().sum())
customers_df.describe()

geo_df.info()

geo_df.nunique()

print("Total Duplication: ",geo_df.duplicated().sum())
geo_df.describe()

OrderItems_df.info()

OrderItems_df.nunique()

print("Total Duplication: ",OrderItems_df.duplicated().sum())
OrderItems_df.describe()

OrderPayments_df.info()

OrderPayments_df.nunique()

print("Total Duplication: ",OrderPayments_df.duplicated().sum())
OrderPayments_df.describe()

OrderReviews_df.info()

OrderReviews_df.isna().sum()

print("Total Duplication: ",OrderReviews_df.duplicated().sum())
OrderReviews_df.describe()

Orders_df.info()

Orders_df.isna().sum()

print("Total Duplication: ",Orders_df.duplicated().sum())
Orders_df.describe()

ProductCat_df.info()

ProductCat_df.nunique()

print("Total Duplication: ",ProductCat_df.duplicated().sum())
ProductCat_df.describe()

Products_df.info()

Products_df.isna().sum()

print("Total Duplication: ",Products_df.duplicated().sum())
Products_df.describe()

Sellers_df.info()

Sellers_df.nunique()

print("Total Duplication: ",Sellers_df.duplicated().sum())
Sellers_df.describe()

"""We already performed a comprehensive assesment of multiple DataFrames. Here's a summary of the key findings:

1. Customers_df;

  a. Data Types: customer_zip_code_prefix is an integer, and the rest are objects (likely strings);

  b. Distinct Values: Varies for each column, with customer_id being unique for each entry;

  c. Missing Values: No missing values;

  d. Duplicates: No duplicate rows.

2. Geo_df;

    a. Data Types: geolocation_lat and geolocation_lng are float64, and the rest are objects;

    b. Distinct Values: Varies for each column;

    c. Missing Values: No missing values;

    d. Duplicates: 261831.

3. OrderItems_df;

    a. Data Types: order_item_id is an integer, and the rest are objects or floats;

    b. Distinct Values: Varies for each column;

    c. Missing Values: No missing values;

    d.Duplicates: No duplicate rows.

4. OrderPayments_df;

    a. Data Types: payment_value is a float64, and the rest are objects or integers;

    b. Distinct Values: Varies for each column;

    c. Missing Values: No missing values;

    d. Duplicates: No duplicate rows.

5. OrderReviews_df;

    a. Data Types: review_score is an integer, and the rest are objects;

    b. Distinct Values: Varies for each column;

    c. Missing Values: Significant missing values in review_comment_title and review_comment_message;
    
    d. Duplicates: No duplicate rows.

6. Orders_df;

    a. Data Types: All columns are objects, incorrect datatype in order_purchase_timestamp, order_approved_at, order_delivered_carrier_date, order_delivered_customer_date, order_estimated_delivery_date;

    b. Distinct Values: Varies for each column;

    c. Missing Values: Missing values in order_approved_at, order_delivered_carrier_date, order_delivered_customer_date;

    d. Duplicates: No duplicate rows.

7. ProductCat_df;

    a. Data Types: All columns are objects;

    b. Distinct Values: Varies for each column;

    c. Missing Values: No missing values;

    d. Duplicates: No duplicate rows.

8. Products_df;

    a. Data Types: product_name_length, product_description_length, product_photos_qty, product_weight_g, product_length_cm, product_height_cm, product_width_cm are float64; the rest are objects;

    b. Distinct Values: Varies for each column;

    c. Missing Values: Missing values in various columns;

    d. Duplicates: No duplicate rows.

9. Sellers_df;

    a. Data Types: seller_zip_code_prefix is an integer, and the rest are objects;

    b. Distinct Values: Varies for each column;

    c. Missing Values: No missing values;
    
    d. Duplicates: No duplicate rows.

Overall Observations:

1. Pay attention to missing values, especially in OrderReviews_df and Products_df;

2. Data types seem appropriate for each column;

3. No significant issues with duplicates across the datasets.


These observations provide a starting point for data cleansing and exploration, addressing missing values where necessary, and understanding the distribution of numeric columns in more detail.

### Data Cleansing

From data assesment we found some task for cleansing:

1. Drop duplicate data from geo_df table;
2. Manipulated missing value on Orders_df and Products_df table;
3. Manipulated datatype on Orders_df.
"""

geo_df[geo_df.duplicated()]

geo_df.drop_duplicates(inplace=True)
print("Total Duplication: ", geo_df.duplicated().sum())

OrderReviews_df.isna().sum()

OrderReviews_df[OrderReviews_df.review_comment_title.isna()]

OrderReviews_df['review_comment_title'].fillna('', inplace=True)

OrderReviews_df[OrderReviews_df.review_comment_message.isna()]

OrderReviews_df['review_comment_message'].fillna('', inplace=True)

OrderReviews_df.isna().sum()

Orders_df.info()

datetime_columns = ["order_purchase_timestamp", "order_approved_at", "order_delivered_carrier_date", "order_delivered_customer_date", "order_estimated_delivery_date"]
for column in datetime_columns:
  Orders_df[column] = pd.to_datetime(Orders_df[column])

Orders_df.info()

Orders_df.head()

Orders_df.isna().sum()

Orders_df['order_approved_at'].fillna(Orders_df['order_estimated_delivery_date'], inplace=True)

Orders_df['order_delivered_carrier_date'].fillna(Orders_df['order_estimated_delivery_date'], inplace=True)

Orders_df['order_delivered_customer_date'].fillna(Orders_df['order_estimated_delivery_date'], inplace=True)

Orders_df.isna().sum()

Products_df.isna().sum()

Products_df[Products_df.product_category_name.isna()]

Products_df.dropna(inplace=True)

Products_df.isna().sum()

"""---


## Exploratory Data Analysis (EDA)


---

We explore all table
"""

customers_df.sample(10)

customers_df.describe(include="all")

customers_df.groupby(by="customer_city").customer_id.nunique().sort_values(ascending=False)

customers_df.groupby(by="customer_state").customer_id.nunique().sort_values(ascending=False)

geo_df.sample(10)

geo_df.describe(include="all")

OrderItems_df.sample(10)

OrderItems_df.describe(include="all")

OrderItems_df.groupby(by="product_id").agg({
    "order_id": "nunique",
    "price": ["max", "min", "mean", "std"]
})

OrderPayments_df.sample(10)

OrderPayments_df.describe(include="all")

OrderReviews_df.sample(10)

OrderReviews_df.describe(include="all")

Orders_df.sample(10)

Orders_df.describe(include="all")

ProductCat_df.sample(10)

ProductCat_df.describe(include="all")

Products_df.sample(10)

Products_df.describe(include="all")

Sellers_df.sample(10)

Sellers_df.describe(include="all")

"""We join customers_df with Orders_df into CustOrder_df"""

CustOrder_df        = pd.merge(
          left      = Orders_df,
          right     = customers_df,
          how       = "left",
          left_on   = "customer_id",
          right_on  = "customer_id"
)
CustOrder_df.head()

"""We grouping CustOrder_df by customer_city"""

CustOrder_df.groupby(by="customer_city").order_id.nunique().sort_values(ascending=False).head(15)

"""We grouping CustOrder_df by customer_state"""

CustOrder_df.groupby(by="customer_state").order_id.nunique().sort_values(ascending=False)

"""We grouping CustOrder_df by order_status"""

CustOrder_df.groupby(by="order_status").order_id.nunique().sort_values(ascending=False)

"""We join OrderItems_df with Products_df into PrOrder_df"""

PrOrder_df      = pd.merge(
      left      = OrderItems_df,
      right     = Products_df,
      how       = "left",
      left_on   = "product_id",
      right_on  = "product_id"
)
PrOrder_df.head()

"""We grouping PrOrder_df by product_category_name"""

PrOrder_df.groupby(by="product_category_name").agg({
    "seller_id" : "nunique",
    "order_id"  : "nunique",
    "price"     : "sum"
})

"""We join PrOrder_df with CustOrder_df into all_df"""

all_df        = pd.merge(
    left      = PrOrder_df,
    right     = CustOrder_df,
    how       = "left",
    left_on   = "order_id",
    right_on  = "order_id"
)
all_df.head()

"""We grouping all_df by customer_city and product_category_name"""

all_df.groupby(by=["customer_city", "product_category_name"]).agg({
                  "price": "sum"
})

"""We grouping all_df by customer_state and product_category_name"""

all_df.groupby(by=["customer_state", "product_category_name"]).agg({
                  "price": "sum"
})

"""We grouping all_df by order_status and product_category_name"""

all_df.groupby(by=["order_status", "product_category_name"]).agg({
                  "order_id": "nunique",
}).sort_values(by="order_status",  ascending=False)

"""We create new dataset for dashboard"""

all_df.to_csv("ds_data.csv",index=False)

"""Here's a summary of the major steps:


1. Exploration of Customers_df;

    a. Sample of customers_df;
    
    b. Descriptive statistics for customers_df;

    c. Grouping by customer_city and customer_state to count unique customer_ids.


2. Exploration of Geo_df;

    a. Sample of geo_df;

    b. Descriptive statistics for geo_df;


3. Exploration of OrderItems_df;

    a. Sample of OrderItems_df;

    a. Descriptive statistics for OrderItems_df;

    b. Aggregating information about product orders, including the number of unique orders, and statistical information about prices, grouped by product_id.


4. Exploration of OrderPayments_df;

    a. Sample of OrderPayments_df;

    b. Descriptive statistics for OrderPayments_df.


5. Exploration of OrderReviews_df;

    a. Sample of OrderReviews_df;

    b. Descriptive statistics for OrderReviews_df.


6. Exploration of Orders_df;

    a. Sample of Orders_df;

    b. Descriptive statistics for Orders_df.


7. Exploration of ProductCat_df;

    a. Sample of ProductCat_df;
    
    b. Descriptive statistics for ProductCat_df.


8. Exploration of Products_df;

    a. Sample of Products_df;
    
    b. Descriptive statistics for Products_df.


9. Exploration of Sellers_df;

    a. Sample of Sellers_df;

    b. Descriptive statistics for Sellers_df.


10. Data Joining and Grouping;

    a. Merging customers_df with Orders_df to create CustOrder_df;

    b. Grouping CustOrder_df by customer_city, customer_state, and order_status to get insights into order counts.


11. Joining and Grouping for Products;

    a. Merging OrderItems_df with Products_df to create PrOrder_df;

    b. Grouping PrOrder_df by product_category_name to analyze unique sellers, unique orders, and total prices.


12. Final Data Integration;

    a. Merging PrOrder_df with CustOrder_df to create all_df;
    
    b. Grouping all_df by customer_city, product_category_name, customer_state, order_status to analyze aggregated prices and order counts.


13. Exporting Data; Exporting the final dataset, all_df, to a CSV file named "ds_data.csv" for potential dashboard usage.

---


## Visualization & Explanatory Analysis


---

### Business Performance
"""

all_df.sample(10)

Orders_df.head()

MonthlyOrders_df                = all_df.resample(
              rule              = 'M',
              on                = 'order_estimated_delivery_date').agg(
                                  {"order_id": "nunique","price": "sum"
})
MonthlyOrders_df.index          = MonthlyOrders_df.index.strftime('%Y-%B')
MonthlyOrders_df.rename(columns = {
                                    "order_id": "order_count",
                                    "price": "revenue"
},                      inplace = True)
MonthlyOrders_df.head()

MonthlyOrders_df.index  = pd.to_datetime(MonthlyOrders_df.index)

plt.figure(figsize      = (9,4))

plt.plot(
    MonthlyOrders_df.index,
    MonthlyOrders_df["order_count"],
    marker              = 'o',
    linewidth           = 3,
    color="#355E3B"
)

MonthlyOrders_df.index  = pd.to_datetime(MonthlyOrders_df.index, format='%B %Y')

plt.title("Order Performance (2016-2018)", loc="center", fontsize=15)
plt.xlabel("Year", fontsize                   = 12)
plt.ylabel("Order", fontsize                  = 12)

plt.yticks(fontsize                           =10)

plt.show()

"""From the provided graph, it is evident that the peak in order count was observed in August 2018. Additionally;


1. Identification of Peak and Decline Periods;

    a. The analysis of the graph reveals a noticeable peak in order count during August 2018. This peak indicates a period of exceptionally high sales or order activity;
    
    b. Conversely, the statement points out notable declines in order numbers during specific months: January 2018, April 2018, June 2018, and October 2018. These months experienced a significant drop in order volume compared to the observed peak.


2. Implications for Business Performance;

    a. The statement emphasizes that the identified fluctuations in order counts have implications for the overall business performance;
    
    b. The peak in August 2018 may suggest a successful period, possibly influenced by seasonal factors, promotions, or other positive business conditions;
    
    c. The declines in order numbers during certain months raise concerns about potential challenges or issues affecting business performance during those periods.


3. Call to Action;

    a. The concluding part of the statement emphasizes the need for action to bring business performance back on track;
    
    b. The recognition of declines in order numbers prompts a proactive response, suggesting that corrective measures or strategic adjustments are required to address the challenges identified in the specified months.


4. Potential Action Steps;

    a. The specific actions to be taken would depend on the underlying causes of the order declines;
    
    b. Businesses might consider conducting a more detailed analysis to understand the factors contributing to the declines, such as market trends, customer behavior, competitor activities, or internal operational issues;
    
    c. Possible actions could include adjusting marketing strategies, launching promotions, optimizing inventory management, enhancing customer engagement, or addressing any identified operational inefficiencies.


5.  Continuous Monitoring and Adaptation;

    a. The statement implies a dynamic approach to business management, where continuous monitoring of performance metrics is essential;

    b. Implementing corrective actions should be followed by ongoing assessment and adaptation to ensure sustained improvements in business performance.

**In summary, the statement serves as a call to action based on the observed patterns in order counts, emphasizing the need for strategic interventions to address declines and enhance overall business performance.**

### Revenue Growth
"""

MonthlyOrders_df.index  = pd.to_datetime(MonthlyOrders_df.index)

plt.figure(figsize      = (9, 4))

plt.plot(
    MonthlyOrders_df.index,
    MonthlyOrders_df["revenue"],
    marker              = 'o',
    linewidth           = 3,
    color               = "#FFFF00"
)

MonthlyOrders_df.index  = pd.to_datetime(MonthlyOrders_df.index, format='%B %Y')

plt.title("Revenue Stream in (2016-2018)", loc="center", fontsize=15)
plt.xlabel("Year", fontsize                   =12)
plt.ylabel("Revenue", fontsize                =12)

plt.yticks(fontsize                           =10)

plt.show()

"""We need Investigate factors that might have contributed to the observed trends. For example, promotions, discounts, or special events can influence both order count and revenue. Consider external factors or events that might have influenced customer behavior and, consequently, the revenue stream. For instance, holiday seasons, marketing campaigns, or changes in economic conditions can impact of Revenue Growth.

### Customer Distribution
"""

bystate_df  = all_df.groupby(
    by      ="customer_state").customer_id.nunique().reset_index()
bystate_df.rename(columns={##justfeds
    "customer_id": "customer_count"
}, inplace  =True)
bystate_df  = bystate_df.sort_values(by="customer_count", ascending=False)
bystate_df

plt.figure(figsize= (9, 4))
colors_           = ["#FFA500"]
sns.barplot(
    x             = "customer_count",
    y             = "customer_state",
    data=bystate_df.sort_values(by="customer_count", ascending=False),
    palette=colors_
)
plt.title(" Total Customer by State", loc="center", fontsize=14)
plt.ylabel("State",fontsize         =12)
plt.xlabel("Customer",fontsize      =12)
plt.tick_params(axis='y',labelsize  =8)
plt.show()

"""Based on the data in the Total Customer by State graph:


1. Identification of Top States;

    a. The graph highlights that São Paulo (SP) has the highest total number of customers;

    b. Following São Paulo, the state of Rio de Janeiro (RJ) comes in second place in terms of the total number of customers;

    c. Minas Gerais (MG) is identified as the state ranking third in terms of total customer count.


2. Recognition of the Lowest State; In contrast, Roraima (RR) is mentioned as the state with the lowest total number of customers.


3. Interpretation of Rankings; The rankings are determined based on the overall customer count for each state, suggesting the popularity or prevalence of the business's customer base in different regions.


4. Implications for Business Strategy;

    a. For businesses, this information can be valuable in shaping marketing strategies, resource allocation, and regional focus;

    b. The higher customer count in certain states may indicate a stronger market presence or demand, suggesting areas where the business can potentially expand or intensify its efforts.


5. Consideration for Regional Dynamics; Understanding the distribution of customers by state is crucial for businesses operating in different regions, as consumer behavior, preferences, and market dynamics can vary significantly.


**In summary, the statement provides a concise summary of the key findings from the "Total Customer by State" graph, emphasizing the states with the highest and lowest customer counts. This information can guide strategic decision-making and resource allocation for the business, taking into account regional variations in customer engagement.**

### Product Performance
"""

byorder_df = all_df.groupby(by="product_category_name").order_id.nunique().reset_index()
byorder_df.rename(columns={
    "order_id": "order_count"
}, inplace=True)
byorder_df = byorder_df.sort_values(by="order_count", ascending=False)
byorder_df

### Sort the data frame by the number of orders, descending;
byorder_df          = byorder_df.sort_values(by="order_count", ascending=False)

### Import top 10 of products;
top_10_products     = byorder_df.head(10)

### Import bottom 10 of products;
bottom_10_products  = byorder_df.tail(10)

### Setup plot size;
plt.figure(figsize  = (10, 5))

### Setup Graph for top 10 of products;
plt.subplot(1, 2, 1)
sns.barplot(
    x               = "order_count",
    y               = "product_category_name",
    data            = top_10_products,
    palette         = "Blues_r"  ### Color
)
plt.title("Top 10 Products by Order Count")
plt.xlabel("Order Count")
plt.ylabel(None)

### Setup Graph for bottom 10 of products;
plt.subplot(1, 2, 2)
sns.barplot(
    x               = "order_count",
    y               = "product_category_name",
    data            = bottom_10_products,
    palette         = "Reds_r"  ### Color
)
plt.title("Bottom 10 Products by Order Count")
plt.xlabel("Order Count")
plt.ylabel(None)

### Sets the distance between subplots
plt.tight_layout()

### Result
plt.show()

"""Let's delve deeper into the analysis of the best and worst performing products based on the provided graph.


**Top Performing Products (Top 3):**

1. "cama_mesa_banho" (Bed, Table, and Bath);

    a.This product category stands out as the best-performing, indicating a high demand for household and bedding items.

    b. The substantial number of orders (9417) suggests that customers find this category particularly appealing.


2. "beleza_saude" (Beauty and Health); With 8836 orders, the beauty and health category demonstrates strong performance, implying a significant interest in health and beauty-related products.


3. "esporte_lazer" (Sports and Leisure);

    a. Sporting and leisure goods, represented by this category, also show robust performance with 7720 orders;

    b. This indicates a keen interest from customers in sports and recreational products.



**Least Performing Products (Bottom 3):**

1. "seguros_e_servicos" (Insurance and Services);

    a. This category is identified as the least popular, with only 2 orders.

    b. The low order count suggests that there may be challenges or limited demand for insurance and service-related products.


2. "pc_gamer" (Gaming PCs);

    a. Gaming PCs, represented by this category, have a relatively low order count of 8, indicating a niche market or potential areas for improvement.


3.  "fashion_roupa_infanto_juvenil" (Fashion for Children and Teens);

    a. The fashion category for children and teens also falls in the bottom performers with 8 orders;

    b. Understanding the factors contributing to lower demand in this segment could guide future marketing strategies.



**Visual Representation:**

- Insights for Action;

    a. For top-performing products, such as "cama_mesa_banho," the company may consider expanding marketing efforts, exploring related product lines, or ensuring ample stock availability;

    b. Least-performing products, like "seguros_e_servicos," may require a more in-depth analysis to understand customer preferences, potentially leading to product modifications, targeted marketing, or discontinuation.




**Business Implications:**


1. Strategic Focus; The analysis enables the company to strategically focus resources on successful product categories while addressing challenges in less popular segments.

2. Market Dynamics; Understanding which products resonate most with customers provides insights into market dynamics and helps tailor strategies to meet consumer demands.

3. Adaptation and Improvement; Continuous monitoring of product performance allows for prompt adaptation and improvement, ensuring sustained business success.


**In summary, this detailed analysis empowers the company to make informed decisions, capitalize on strengths, and address weaknesses in its product offerings.**

### **RFM analysis**
"""

all_df.sample(10)

rfm_df = all_df.groupby(by="customer_id", as_index=False).agg({
    "order_purchase_timestamp": "max",
    "order_id"                : "nunique",
    "price"                   : "sum"
})
rfm_df.columns            = ["customer_id", "order_purchase_timestamp",
                             "frequency", "monetary"]
rfm_df.head()

rfm_df["order_purchase_timestamp"] = rfm_df["order_purchase_timestamp"].dt.date
recent_date = Orders_df["order_purchase_timestamp"].dt.date.max()
rfm_df["recency"] = rfm_df["order_purchase_timestamp"].apply(
    lambda x: (recent_date - x).days)
rfm_df.head()

rfm_df.drop("order_purchase_timestamp", axis=1, inplace=True)
rfm_df.head()

rfm_df.describe()

rfm_df.sort_values(by="recency", ascending=True).head(10)

rfm_df.sort_values(by="frequency", ascending=False).head(10)

fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(35, 5))

colors = ["#FFDF00"]

sns.barplot(y="recency", x="customer_id", data=rfm_df.sort_values(by="recency",
                            ascending=True).head(5), palette=colors, ax=ax[0])
ax[0].set_ylabel(None)
ax[0].set_xlabel(None)
ax[0].set_title("By Recency (days)", loc="center", fontsize=15)
ax[0].tick_params(axis ='x', labelsize=15, rotation=90)

sns.barplot(y="frequency", x="customer_id", data=rfm_df.sort_values(
    by="frequency", ascending=False).head(5), palette=colors, ax=ax[1])
ax[1].set_ylabel(None)
ax[1].set_xlabel(None)
ax[1].set_title("By Frequency", loc="center", fontsize=15)
ax[1].tick_params(axis='x', labelsize=15, rotation=90)

sns.barplot(y="monetary", x="customer_id", data=rfm_df.sort_values(
    by="monetary", ascending=False).head(5), palette=colors, ax=ax[2])
ax[2].set_ylabel(None)
ax[2].set_xlabel(None)
ax[2].set_title("By Monetary", loc="center", fontsize=15)
ax[2].tick_params(axis='x', labelsize=15, rotation=90)

plt.suptitle("RFM Parameters by customer_id", fontsize=23)
plt.show()

rfm_df['r_rank'] = rfm_df['recency'].rank(ascending=False)
rfm_df['f_rank'] = rfm_df['frequency'].rank(ascending=True)
rfm_df['m_rank'] = rfm_df['monetary'].rank(ascending=True)

rfm_df.head()

"""**Normalize Customer Ratings**"""

rfm_df['r_rank_norm'] = (rfm_df['r_rank']/rfm_df['r_rank'].max())*100
rfm_df['f_rank_norm'] = (rfm_df['f_rank']/rfm_df['f_rank'].max())*100
rfm_df['m_rank_norm'] = (rfm_df['m_rank']/rfm_df['m_rank'].max())*100

rfm_df.drop(columns=['r_rank', 'f_rank', 'm_rank'], inplace=True)

rfm_df.head()

rfm_df['RFM_score'] = 0.15*rfm_df['r_rank_norm']+0.28 * \
rfm_df['f_rank_norm']+0.57*rfm_df['m_rank_norm']
rfm_df['RFM_score'] *= 0.05
rfm_df = rfm_df.round(2)
rfm_df[['customer_id', 'RFM_score']].head(7)

rfm_df["customer_segment"] = np.where(
rfm_df['RFM_score'] > 4.5, "Top customers", (np.where(
rfm_df['RFM_score'] > 4, "High value customer",(np.where(
rfm_df['RFM_score'] > 3, "Medium value customer", np.where(
rfm_df['RFM_score'] > 1.6, 'Low value customers', 'Lost customers'))))))

rfm_df[['customer_id', 'RFM_score', 'customer_segment']].head(20)

customer_segment_df = rfm_df.groupby(by="customer_segment",
                                     as_index=False).customer_id.nunique()
customer_segment_df

customer_segment_df['customer_segment'] = pd.Categorical (customer_segment_df[
    'customer_segment'], [
    "Lost customers", "Low value customers", "Medium value customer",
    "High value customer", "Top customers"
])

plt.figure(figsize=(10, 5))
colors_ = ["#7F00FF"]

sns.barplot(
    x="customer_id",
    y="customer_segment",
    data=customer_segment_df.sort_values(by="customer_segment", ascending=False),
    palette=colors_
)
plt.title("Segmented Total Customer", loc="center", fontsize=15)
plt.ylabel(None)
plt.xlabel(None)
plt.tick_params(axis='y', labelsize=12)
plt.show()

"""This data provides a comprehensive segmentation of the total customer base, categorizing customers into five distinct groups based on their value and level of engagement. The segmentation is a strategic approach to better understand and manage the diverse customer landscape. Let's elaborate on the key points presented in the statement:


1. Lost Customers;

    a. Definition : Customers who were previously engaged with the company but have discontinued their use of products or service;

    b. Reasons    : They may have switched to competitors or ended their relationship with the company for various reasons;

    c. Strategies : The company should develop strategies to re-engage lost customers, potentially through targeted marketing or personalized incentives.


2. Low-Value Customers;

    a. Definition : Customers with relatively low value or profitability for the company;

    b. Indicators : Infrequent purchases or a low average transaction value characterize this group;

    c. Strategies : Opportunities exist to increase their value, perhaps through promotional offers, loyalty programs, or personalized marketing campaigns.


3. Medium-Value Customers;

    a. Definition : Customers with a moderate level of value for the company;

    b. Indicators : Regular purchases and a decent average transaction value;

    c. Strategies : The focus here is on maintaining and potentially enhancing their engagement to move them into higher-value categories.


4. High-Value Customers;

    a. Definition : Customers with a high level of value and profitability for the company;

    b. Indicators : Frequent purchases and a high average transaction value;

    c. Strategies : These valuable customers should be retained and nurtured, possibly through exclusive offers, premium services, or personalized experiences.


5. Top Customers;

    a. Definition : The most valuable customers for the company;

    b. Indicators : Highest level of value and profitability, with long-standing relationships and frequent high-value purchases;

    c. Strategies : Given their significant potential for revenue generation, top customers should be a priority for retention efforts, personalized services, and exclusive benefits.



- **Strategic Implications:**

    a. Identifying lost customers allows the company to implement strategies for customer recovery;

    b. Analyzing low and medium-value customer behaviors presents opportunities for value enhancement and increased engagement;

    c. Prioritizing efforts on retaining and nurturing high-value and top customers aligns with maximizing revenue potential.


**In conclusion, the data underscores the importance of customer segmentation and provides actionable insights for strategic decision-making. Understanding the distribution of customers across these segments enables the company to tailor specific strategies for each category, ultimately optimizing customer value and retention.**

---


## Conclusion


---

1.  Business performance: In summary, the conclusion serves as a call to action based on observed patterns in order counts, emphasizing the need for strategic interventions to address declines and enhance overall business performance. The recommendation for continuous monitoring reflects a commitment to adaptability and ongoing improvement.


2.  Revenue growth: In summary, the conclusion from the statement is that a thorough investigation into factors influencing trends, including promotions, special events, and external influences, is essential for a nuanced understanding of order count and revenue dynamics. This understanding can guide strategic decisions and enhance overall revenue growth.


3.  Customer distribution: In summary, the statement provides a comprehensive overview of the state-wise distribution of customers, emphasizing the significance of this information for guiding strategic decisions and resource allocation in a region-specific context.


4.  Product performance: In summary, this detailed analysis empowers the company to make informed decisions, capitalize on strengths, and address weaknesses in its product offerings.


- **RFM Analysis: In conclusion, the data underscores the importance of customer segmentation, providing actionable insights for strategic decision-making. Understanding the distribution of customers across segments enables the tailoring of specific strategies for each category, ultimately optimizing customer value and retention. This approach is vital for enhancing overall business performance and ensuring sustained success.**
"""